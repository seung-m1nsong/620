{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObriK_kQ7fFC"
      },
      "source": [
        "# **Data 620 Project 3**\n",
        "Seung Min Song, Krutika Patel<br>\n",
        "\n",
        "03/31/2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Project 3**\n",
        "\n",
        "Using any of the three classifiers described in this chapter, and any features you can think of, build the best name gender classifier you can. \n",
        "\n",
        "Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6,900 words for the training set. \n",
        "\n",
        "Then, starting with the example name gender classifier, make incremental improvements. \n",
        "\n",
        "Use the devtest set to check your progress. \n",
        "\n",
        "Once you are satisfied with your classifier, check its final performance on the test set. \n",
        "\n",
        "How does the performance on the test set compare to the performance on the dev-test set? Is this what you’d expect?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Supervised Classification\n",
        "\n",
        "1. Training: Materials with correct answers (=already classified) → pattern learning\n",
        "    * Feature: Criteria for identifying and describing patterns in data\n",
        "    * Algorithm: A method of calculating classification results from feature values\n",
        "\n",
        "2. Test = Prediction: Learned pattern → Classify new data\n",
        "Gender Identification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gender Identification\n",
        "\n",
        "### Name gender classification: feature extraction\n",
        "\n",
        "Pattern:\n",
        "* Ends with pattern a,e,i → female\n",
        "* Ends with k,o,r,s,t → male\n",
        "\n",
        "Feature:\n",
        "* What is the last letter of the feature name?\n",
        "\n",
        "Function definition: \n",
        "* Select the feature value of a given name (variable name: gender_features)\n",
        "\n",
        "Input:\n",
        "* string (variable name word)\n",
        "\n",
        "output\n",
        "* dictionary\n",
        "\n",
        "Key: \n",
        "* last_letter\n",
        "\n",
        "Value: \n",
        "* word[-1] (last one letter of word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gender_features(word):\n",
        "    return {'last_letter': re.sub('[0-9]', '', word)[-1].lower()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Cordelie', 'female'), ('Peggie', 'female'), ('Solange', 'female'), ('Rana', 'female'), ('Jessy', 'female'), ('Lelia', 'female'), ('Dorothy', 'female'), ('Ulrick', 'male'), ('Roshelle', 'female'), ('Caitrin', 'female')]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "random.seed(123)  \n",
        "np.random.seed(123)\n",
        "\n",
        "from nltk.corpus import names\n",
        "labeled_names = (\n",
        "[(name, 'male') for name in names.words('male.txt')] +\n",
        "[(name, 'female') for name in names.words('female.txt')])\n",
        "import random\n",
        "random.shuffle(labeled_names)\n",
        "print(labeled_names[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name Gender Classification: Construct a classifier → learn from training set\n",
        "\n",
        "1 Feature set composition: list (variable name featuresets)\n",
        "* Element 2-tuple: (Feature =Dictionary, Label=Gender)\n",
        "* Example ({'last_letter': n}, 'male') | ({'last_letter': e}, 'female')\n",
        "    \n",
        "     ▶ Note: Specific names such as Aaron and Zoe are reduced to qualities.\n",
        "\n",
        "2 Corpus partitioning\n",
        "* Test set: first 500\n",
        "* Development Test Set: Next 500:1000]\n",
        "* Training Set: Rest\n",
        "    \n",
        "3 Classifier (variable name classifier)\n",
        "* Algorithm Naive Bayes: nltk.NaiveBayesClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name Gender Classification: Classifier Performance Evaluation\n",
        "\n",
        "* Function: nltk.classify.accuracy()\n",
        "* Input: classifier, experiment set\n",
        "* Output: accuracy\n",
        "* Result 0.78 → higher than 0.5 that would be guessed by chance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the development test set:  0.78\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# featuresets \n",
        "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
        "\n",
        "test_set = featuresets[:500]                   \n",
        "dev_test_set = featuresets[500:1000]           \n",
        "train_set = featuresets[1000:]                \n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "accuracy = nltk.classify.accuracy(classifier, dev_test_set)\n",
        "print(\"Accuracy on the development test set: \", nltk.classify.accuracy(classifier, dev_test_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name gender classification: Apply classifier\n",
        "\n",
        "Function: classifier.classify()\n",
        "* Input: feature dictionary\n",
        "* Output: label=gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'female'"
            ]
          },
          "execution_count": 232,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.classify(gender_features('Ahley'))\n",
        "#classifier.classify(gender_features('John'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name Gender Classification: Review Features\n",
        "\n",
        "* Featyre: information quantity evaluation\n",
        "* Function: classifier.show_most_informative_features()\n",
        "* Inputs: number of n\n",
        "* Print: Top n qualities with high output information amount\n",
        "* Example: Ending with a, the probability of being female is 36 times higher than the probability of being male."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "             last_letter = 'a'            female : male   =     33.3 : 1.0\n",
            "             last_letter = 'k'              male : female =     29.2 : 1.0\n",
            "             last_letter = 'p'              male : female =     18.6 : 1.0\n",
            "             last_letter = 'f'              male : female =     15.2 : 1.0\n",
            "             last_letter = 'v'              male : female =      9.8 : 1.0\n",
            "             last_letter = 'd'              male : female =      9.8 : 1.0\n",
            "             last_letter = 'm'              male : female =      9.2 : 1.0\n",
            "             last_letter = 'o'              male : female =      8.0 : 1.0\n",
            "             last_letter = 'w'              male : female =      8.0 : 1.0\n",
            "             last_letter = 'r'              male : female =      6.7 : 1.0\n"
          ]
        }
      ],
      "source": [
        "classifier.show_most_informative_features()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement a function that predicts gender and returns the result as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(usernames):\n",
        "    return [{u: classifier.classify(gender_features(u))} for u in usernames]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_data = [\n",
        "    'Trump', \n",
        "    'Ashley',\n",
        "    'Biden',\n",
        "    'David',\n",
        "    'Seungmin',\n",
        "    'Krutika',     \n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Executing the predict() function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'Trump': 'male'},\n",
              " {'Ashley': 'female'},\n",
              " {'Biden': 'male'},\n",
              " {'David': 'male'},\n",
              " {'Seungmin': 'male'},\n",
              " {'Krutika': 'female'}]"
            ]
          },
          "execution_count": 237,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict(input_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name Gender Category: Performance Comparison\n",
        "\n",
        "gender_features() vs gender_features2()\n",
        "1. Construct a feature set based on gender_features2()\n",
        "2. Train a new classifier from the same training set\n",
        "3. Apply the new classifier to the same set of experiments\n",
        "4. Accuracy rating: 0.78 > 0.782\n",
        "\n",
        "Feature have been added and accuracy is increased."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def gender_features2(name):\n",
        "    name_lower = name.lower() \n",
        "    features = {\n",
        "        'first_letter': name_lower[0],\n",
        "        'last_letter': name_lower[-1],\n",
        "        **{'count({})'.format(letter): name_lower.count(letter) for letter in string.ascii_lowercase},\n",
        "        **{'has({})'.format(letter): (letter in name_lower) for letter in string.ascii_lowercase}\n",
        "    }\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the development test set:  0.782\n"
          ]
        }
      ],
      "source": [
        "featuresets = [(gender_features2(n), gender) \n",
        "            for (n, gender) in labeled_names]\n",
        "\n",
        "test_set = featuresets[:500]  \n",
        "dev_test_set = featuresets[500:1000] \n",
        "train_set = featuresets[1000:]  \n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "accuracy = nltk.classify.accuracy(classifier, dev_test_set)\n",
        "print(\"Accuracy on the development test set: \", nltk.classify.accuracy(classifier, dev_test_set))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When vowels and consonants were included, the number reduced from 0.782 to 0.788"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def gender_features3(name):\n",
        "    name_lower = name.lower()\n",
        "    vowels = 'aeiou'\n",
        "    consonants = ''.join(set(string.ascii_lowercase) - set(vowels))\n",
        "    \n",
        "    num_vowels = sum(name_lower.count(v) for v in vowels)\n",
        "    num_consonants = sum(name_lower.count(c) for c in consonants)\n",
        "    \n",
        "    features = {\n",
        "        'first_letter': name_lower[0],\n",
        "        'last_letter': name_lower[-1],\n",
        "        'num_vowels': num_vowels,  \n",
        "        'num_consonants': num_consonants,  \n",
        "        **{'count({})'.format(letter): name_lower.count(letter) for letter in string.ascii_lowercase},\n",
        "        **{'has({})'.format(letter): (letter in name_lower) for letter in string.ascii_lowercase},\n",
        "        \n",
        "    }\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the development test set:  0.788\n"
          ]
        }
      ],
      "source": [
        "featuresets = [(gender_features3(n), gender) \n",
        "            for (n, gender) in labeled_names]\n",
        "\n",
        "test_set = featuresets[:500]  \n",
        "dev_test_set = featuresets[500:1000] \n",
        "train_set = featuresets[1000:]  \n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "accuracy = nltk.classify.accuracy(classifier, dev_test_set)\n",
        "print(\"Accuracy on the development test set: \", nltk.classify.accuracy(classifier, dev_test_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Name gender classification: Modify feature\n",
        "\n",
        "* last letter of name\n",
        "* Last 1 letter of name\n",
        "* Last 2 letters of name\n",
        "* length of name\n",
        "\n",
        "As a result of test_set, accuracy improved from 0.796 to 0.81."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gender_features4(name):\n",
        "    name_lower = name.lower() \n",
        "    features = {\n",
        "        'first_letter': name_lower[0],\n",
        "        'last_letter': name_lower[-1],\n",
        "        **{'count({})'.format(letter): name_lower.count(letter) for letter in string.ascii_lowercase},\n",
        "        **{'has({})'.format(letter): (letter in name_lower) for letter in string.ascii_lowercase},\n",
        "        'suffix1': name[-1:], \n",
        "        'suffix2': name[-2:],\n",
        "        #'length': len(name),\n",
        "    }\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the development test set:  0.794\n",
            "Accuracy on the test set:  0.81\n"
          ]
        }
      ],
      "source": [
        "featuresets = [(gender_features4(n), gender) \n",
        "            for (n, gender) in labeled_names]\n",
        "\n",
        "test_set = featuresets[:500]  \n",
        "dev_test_set = featuresets[500:1000] \n",
        "train_set = featuresets[1000:]  \n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "accuracy = nltk.classify.accuracy(classifier, dev_test_set)\n",
        "print(\"Accuracy on the development test set: \", nltk.classify.accuracy(classifier, dev_test_set))\n",
        "\n",
        "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
        "print(\"Accuracy on the test set: \", nltk.classify.accuracy(classifier, test_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test-set vs Dev-test-set\n",
        "\n",
        "Typically, during the process of developing and tuning a model, we continuously check its performance on the development dev-test set and adjust the model. Therefore, it is common for the dev-test set to have better results than the test set. \n",
        "\n",
        "However, higher performance on the test set than on the development test set may indicate that the model is not overfitting on the development test set and has good generalization ability. Because both the development test set and the test set are relatively small in size, there may be more variability in performance evaluations. In other words, small data sets are prone to greater variability in performance measurements."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
