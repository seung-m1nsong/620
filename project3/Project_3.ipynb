{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObriK_kQ7fFC"
      },
      "source": [
        "# **Data 620 Project 3**\n",
        "Seung Min Song, Krutika Patel<br>\n",
        "\n",
        "03/31/2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Project 3**\n",
        "\n",
        "Using any of the three classifiers described in this chapter, and any features you can think of, build the best name gender classifier you can. \n",
        "\n",
        "Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6,900 words for the training set. \n",
        "\n",
        "Then, starting with the example name gender classifier, make incremental improvements. \n",
        "\n",
        "Use the devtest set to check your progress. \n",
        "\n",
        "Once you are satisfied with your classifier, check its final performance on the test set. \n",
        "\n",
        "How does the performance on the test set compare to the performance on the dev-test set? Is this what you’d expect?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Supervised Learning*\n",
        "\n",
        "1. Training: Materials with correct answers (=already classified) → pattern learning\n",
        "    * Feature: Criteria for identifying and describing patterns in data\n",
        "    * Algorithm: A method of calculating classification results from feature values\n",
        "\n",
        "2. Test = Prediction: Learned pattern → Classify new data\n",
        "Gender Identification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gender Identification\n",
        "\n",
        "### Name gender classification: feature extraction\n",
        "\n",
        "Pattern:\n",
        "* Ends with pattern a,e,i → female\n",
        "* Ends with k,o,r,s,t → male\n",
        "\n",
        "Feature:\n",
        "* What is the last letter of the feature name?\n",
        "\n",
        "Function definition: \n",
        "* Select the feature value of a given name (variable name: gender_features)\n",
        "\n",
        "Input:\n",
        "* string (variable name word)\n",
        "\n",
        "output\n",
        "* dictionary\n",
        "\n",
        "Key: \n",
        "* last_letter\n",
        "\n",
        "Value: \n",
        "* word[-1] (last one letter of word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 412,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from nltk.corpus import names\n",
        "\n",
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "\n",
        "# 이름 데이터 로드 및 레이블링\n",
        "labeled_names = (\n",
        "    [(name, 'male') for name in names.words('male.txt')] +\n",
        "    [(name, 'female') for name in names.words('female.txt')]\n",
        ")\n",
        "\n",
        "# 데이터 섞기\n",
        "random.shuffle(labeled_names)\n",
        "\n",
        "# 숫자가 포함된 이름을 찾아 출력\n",
        "names_with_digits = [name for name, gender in labeled_names if any(char.isdigit() for char in name)]\n",
        "print(names_with_digits[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 414,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Cordelie', 'female'), ('Peggie', 'female'), ('Solange', 'female'), ('Rana', 'female'), ('Jessy', 'female'), ('Lelia', 'female'), ('Dorothy', 'female'), ('Ulrick', 'male'), ('Roshelle', 'female'), ('Caitrin', 'female')]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from nltk.corpus import names\n",
        "\n",
        "random.seed(123)  \n",
        "np.random.seed(123)\n",
        "\n",
        "from nltk.corpus import names\n",
        "labeled_names = (\n",
        "[(name, 'male') for name in names.words('male.txt')] +\n",
        "[(name, 'female') for name in names.words('female.txt')])\n",
        "import random\n",
        "random.shuffle(labeled_names)\n",
        "print(labeled_names[:10])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name Gender Classification: Construct a classifier → learn from training set\n",
        "\n",
        "1 Feature set composition: list (variable name featuresets)\n",
        "* Element 2-tuple: (Feature =Dictionary, Label=Gender)\n",
        "* Example ({'last_letter': n}, 'male') | ({'last_letter': e}, 'female')\n",
        "    \n",
        "     ▶ Note: Specific names such as Aaron and Zoe are reduced to qualities.\n",
        "\n",
        "2 Corpus partitioning\n",
        "* Test set: first 500\n",
        "* Development Test Set: Next 500:1000]\n",
        "* Training Set: Rest\n",
        "    \n",
        "3 Classifier (variable name classifier)\n",
        "* Algorithm Naive Bayes: nltk.NaiveBayesClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name Gender Classification: Classifier Performance Evaluation\n",
        "\n",
        "* Function: nltk.classify.accuracy()\n",
        "* Input: classifier, experiment set\n",
        "* Output: accuracy\n",
        "* Result 0.78 → higher than 0.5 that would be guessed by chance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the development test set:  0.78\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "def gender_features(word):\n",
        "    return {'last_letter': word[-1].lower()}\n",
        "\n",
        "# featuresets \n",
        "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
        "\n",
        "test_set = featuresets[:500]                   \n",
        "dev_test_set = featuresets[500:1000]           \n",
        "train_set = featuresets[1000:]                \n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "accuracy = nltk.classify.accuracy(classifier, dev_test_set)\n",
        "print(\"Accuracy on the development test set: \", nltk.classify.accuracy(classifier, dev_test_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name gender classification: Apply classifier\n",
        "\n",
        "Function: classifier.classify()\n",
        "* Input: feature dictionary\n",
        "* Output: label=gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 416,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'female'"
            ]
          },
          "execution_count": 416,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.classify(gender_features('Ahley'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name Gender Classification: Review Features\n",
        "\n",
        "* Featyre: information quantity evaluation\n",
        "* Function: classifier.show_most_informative_features()\n",
        "* Inputs: number of n\n",
        "* Print: Top n qualities with high output information amount\n",
        "* Example: Ending with a, the probability of being female is 36 times higher than the probability of being male."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 417,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "             last_letter = 'a'            female : male   =     33.3 : 1.0\n",
            "             last_letter = 'k'              male : female =     29.2 : 1.0\n",
            "             last_letter = 'p'              male : female =     18.6 : 1.0\n",
            "             last_letter = 'f'              male : female =     15.2 : 1.0\n",
            "             last_letter = 'v'              male : female =      9.8 : 1.0\n",
            "             last_letter = 'd'              male : female =      9.8 : 1.0\n",
            "             last_letter = 'm'              male : female =      9.2 : 1.0\n",
            "             last_letter = 'o'              male : female =      8.0 : 1.0\n",
            "             last_letter = 'w'              male : female =      8.0 : 1.0\n",
            "             last_letter = 'r'              male : female =      6.7 : 1.0\n"
          ]
        }
      ],
      "source": [
        "classifier.show_most_informative_features()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement a function that predicts gender and returns the result as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 418,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(usernames):\n",
        "    return [{u: classifier.classify(gender_features(u))} for u in usernames]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 419,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_data = [\n",
        "    'Trump', \n",
        "    'Ashley',\n",
        "    'Biden',\n",
        "    'Kim',\n",
        "    'Mei',\n",
        "    'April', \n",
        "    'Sonny'    \n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Executing the predict() function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 420,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'Trump': 'male'},\n",
              " {'Ashley': 'female'},\n",
              " {'Biden': 'male'},\n",
              " {'Kim': 'male'},\n",
              " {'Mei': 'female'},\n",
              " {'April': 'male'},\n",
              " {'Sonny': 'female'}]"
            ]
          },
          "execution_count": 420,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict(input_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Name gender classification: Modify feature\n",
        "\n",
        "* last letter of name\n",
        "* Last 1 letter of name\n",
        "* The number of occurrences and inclusions of each letter of the alphabet\n",
        "\n",
        "As a result of test_set, accuracy is 0.786."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 426,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the development test set:  0.786\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import string\n",
        "\n",
        "def gender_features2(name):\n",
        "    name_lower = name.lower() \n",
        "    \n",
        "\n",
        "    features = {\n",
        "        'first_letter': name_lower[0],\n",
        "        'first_two_letters': name_lower[:2] if len(name_lower) >= 2 else name_lower[0],\n",
        "        'last_letter': name_lower[-1],\n",
        "        **{'count({})'.format(letter): name_lower.count(letter) for letter in string.ascii_lowercase},\n",
        "        **{'has({})'.format(letter): (letter in name_lower) for letter in string.ascii_lowercase}\n",
        "    }\n",
        "    return features\n",
        "\n",
        "featuresets = [(gender_features2(n), gender) \n",
        "            for (n, gender) in labeled_names]\n",
        "\n",
        "test_set = featuresets[:500]  \n",
        "dev_test_set = featuresets[500:1000] \n",
        "train_set = featuresets[1000:]  \n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "accuracy = nltk.classify.accuracy(classifier, dev_test_set)\n",
        "print(\"Accuracy on the development test set: \", nltk.classify.accuracy(classifier, dev_test_set))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* last letter of name\n",
        "* Last 1 letter of name\n",
        "* Last 2 letters of name\n",
        "* vowels + consonants\n",
        "\n",
        "As a result of test_set, accuracy is 0.816."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 424,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the development test set:  0.816\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import string\n",
        "\n",
        "def gender_features3(name):\n",
        "    name_lower = name.lower()\n",
        "    vowels = 'aeiou'\n",
        "    consonants = ''.join(set(string.ascii_lowercase) - set(vowels))\n",
        "    \n",
        "    num_vowels = sum(name_lower.count(v) for v in vowels)\n",
        "    num_consonants = sum(name_lower.count(c) for c in consonants)\n",
        "    \n",
        "\n",
        "    features = {\n",
        "        'first_letter': name_lower[0],\n",
        "        'first_two_letters': name_lower[:2] if len(name_lower) >= 2 else name_lower[0],\n",
        "        'last_letter': name_lower[-1],\n",
        "        'num_vowels': num_vowels,  \n",
        "        'num_consonants': num_consonants,  \n",
        "       # **{'count({})'.format(letter): name_lower.count(letter) for letter in string.ascii_lowercase},\n",
        "       # **{'has({})'.format(letter): (letter in name_lower) for letter in string.ascii_lowercase},\n",
        "        'suffix2': name[-2:],\n",
        "        \n",
        "    }\n",
        "    return features\n",
        "\n",
        "featuresets = [(gender_features3(n), gender) \n",
        "            for (n, gender) in labeled_names]\n",
        "\n",
        "test_set = featuresets[:500]  \n",
        "dev_test_set = featuresets[500:1000] \n",
        "train_set = featuresets[1000:]  \n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "accuracy = nltk.classify.accuracy(classifier, dev_test_set)\n",
        "print(\"Accuracy on the development test set: \", nltk.classify.accuracy(classifier, dev_test_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Name gender classification: Modify feature\n",
        "\n",
        "* last letter of name\n",
        "* Last 1 letter of name\n",
        "* Last 2 letters of name\n",
        "* length of name\n",
        "\n",
        "As a result of test_set, accuracy is 0.818."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 425,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the development test set:  0.818\n",
            "Accuracy on the test set:  0.798\n"
          ]
        }
      ],
      "source": [
        "def gender_features4(name):\n",
        "    name_lower = name.lower() \n",
        "    features = {\n",
        "        'first_letter': name_lower[0],\n",
        "        'first_two_letters': name_lower[:2] if len(name_lower) >= 2 else name_lower[0],\n",
        "        'last_letter': name_lower[-1],\n",
        "        #**{'count({})'.format(letter): name_lower.count(letter) for letter in string.ascii_lowercase},\n",
        "        #**{'has({})'.format(letter): (letter in name_lower) for letter in string.ascii_lowercase},\n",
        "        'suffix2': name[-2:],\n",
        "        'length': len(name),\n",
        "    }\n",
        "    return features\n",
        "\n",
        "featuresets = [(gender_features4(n), gender) \n",
        "            for (n, gender) in labeled_names]\n",
        "\n",
        "test_set = featuresets[:500]  \n",
        "dev_test_set = featuresets[500:1000] \n",
        "train_set = featuresets[1000:]  \n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "accuracy = nltk.classify.accuracy(classifier, dev_test_set)\n",
        "print(\"Accuracy on the development test set: \", nltk.classify.accuracy(classifier, dev_test_set))\n",
        "\n",
        "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
        "print(\"Accuracy on the test set: \", nltk.classify.accuracy(classifier, test_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Decision Tree*\n",
        "\n",
        "Modify the code to include additional information in the feature extraction step for the decision tree model.\n",
        "\n",
        "Here, the **first and last letters of the name**, **the number of occurrences and inclusions of each letter of the alphabet**, and **the last 1 letter and last 2 letters of the name** are used as features.\n",
        "\n",
        "Among the several feature extraction methods presented, the combination that showed the highest accuracy on the dev-test set is as follows:\n",
        "\n",
        "'first_letter' and 'last_letter': 0.782\n",
        "\n",
        "'first_letter', 'last_letter', 'length',  added: 0.796\n",
        "\n",
        "These results show that the combination of the first and last letters of the name and the length of the name as features was the most sophisticated (highly accurate) combination in the development test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 438,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the development test set: 0.796\n",
            "Accuracy on the test set: 0.768\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from nltk.corpus import names\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "\n",
        "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
        "                [(name, 'female') for name in names.words('female.txt')])\n",
        "random.shuffle(labeled_names)\n",
        "\n",
        "def gender_features5(name):\n",
        "    name_lower = name.lower()\n",
        "    features = {\n",
        "        'first_letter': name_lower[0],\n",
        "        #'first_two_letters': name_lower[:2] if len(name_lower) >= 2 else name_lower[0],\n",
        "        'last_letter': name_lower[-1],\n",
        "        #**{'count({})'.format(letter): name_lower.count(letter) for letter in string.ascii_lowercase},\n",
        "        #**{'has({})'.format(letter): (letter in name_lower) for letter in string.ascii_lowercase},\n",
        "        #'suffix2': name_lower[-2:],\n",
        "        'length': len(name),\n",
        "    }\n",
        "    return features\n",
        "\n",
        "featuresets = [(gender_features5(n), gender) for (n, gender) in labeled_names]\n",
        "v = DictVectorizer(sparse=False)\n",
        "X = v.fit_transform([feature for feature, gender in featuresets])\n",
        "y = np.array([1 if gender == 'male' else 0 for _, gender in featuresets])\n",
        "\n",
        "X_train, y_train = X[1000:], y[1000:]\n",
        "X_dev_test, y_dev_test = X[500:1000], y[500:1000]\n",
        "X_test, y_test = X[:500], y[:500]\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "accuracy_dev_test = accuracy_score(y_dev_test, clf.predict(X_dev_test))\n",
        "print(\"Accuracy on the development test set:\", accuracy_dev_test)\n",
        "\n",
        "accuracy_test = accuracy_score(y_test, clf.predict(X_test))\n",
        "print(\"Accuracy on the test set:\", accuracy_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Test-set vs Dev-test-set*\n",
        "\n",
        "Typically, during the process of developing and tuning a model, we continuously check its performance on the development dev-test set and adjust the model. Therefore, it is common for the dev-test set to have better results than the test set. \n",
        "\n",
        "However, higher performance on the test set than on the development test set may indicate that the model is not overfitting on the development test set and has good generalization ability. Because both the development test set and the test set are relatively small in size, there may be more variability in performance evaluations. In other words, small data sets are prone to greater variability in performance measurements."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
